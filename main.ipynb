{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find course description from course name at NYU using Course Catalog API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rootkey.json', 'r') as json_file:\n",
    "    rootkey = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, couldn't find any courses like that!\n"
     ]
    }
   ],
   "source": [
    "course_name = \"Data Structures and Algorithms\" # placeholder test course name\n",
    "course_name = course_name.replace(\" \", \"+\")\n",
    "\n",
    "url = 'https://sandbox.api.it.nyu.edu/course-catalog-exp/courses?course_title=' + course_name\n",
    "headers = {'Authorization': rootkey['NYUAccessKey']}\n",
    "\n",
    "req = requests.get(url, headers=headers)\n",
    "\n",
    "# print(req.status_code)\n",
    "# print(req.headers)\n",
    "# print(req.text)\n",
    "\n",
    "course_reqs = json.loads(req.text)\n",
    "allsentences = []\n",
    "if len(course_reqs) == 0:\n",
    "    print(\"Sorry, couldn't find any courses like that!\")\n",
    "else:\n",
    "    for i in range(len(course_reqs)):\n",
    "        course_des = course_reqs[i][\"course_descr\"]\n",
    "        allsentences.append(course_des)\n",
    "\n",
    "# print(allsentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bag of words and create vector for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ericm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'algorithm', 'algorithms', 'allocation', 'along', 'analysis', 'available', 'basics', 'better', 'binary', 'c', 'comparison', 'complexity', 'computational', 'corequisite', 'course', 'covers', 'cs', 'data', 'design', 'dynamic', 'ex', 'fundamental', 'general', 'graduate', 'gy', 'hashing', 'heap', 'implementation', 'include', 'introduces', 'linked', 'mergesort', 'methods', 'not', 'online', 'open', 'prerequisite', 'program', 'queues', 'quicksort', 'searching', 'selection', 'sequential', 'sort', 'sorting', 'specifications', 'stacks', 'standard', 'standing', 'storage', 'structures', 'students', 'taken', 'techniques', 'this', 'topics', 'trees', 'types', 'use', 'uy', 'version']\n",
      "Word List for Document \n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def word_cleanup(sentence):\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in stop_words and w.isalpha()]\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = word_cleanup(sentence)\n",
    "        words.extend(w)\n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def generate_bow(allsentences):    \n",
    "    vocab = tokenize(allsentences)\n",
    "    print(\"Bag of Words for Document \\n{0} \\n\".format(vocab));\n",
    "    \n",
    "generate_bow(allsentences)\n",
    "    \n",
    "for sentence in allsentences:\n",
    "    words = word_cleanup(sentence)\n",
    "    bag_vector = np.zeros(len(vocab))\n",
    "    for w in words:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if word == w: \n",
    "                bag_vector[i] += 1\n",
    "\n",
    "    print(\"{0}\\n{1}\\n\".format(sentence,np.array(bag_vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Comprehend Detect Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rootkey.json', 'r') as json_file:\n",
    "    rootkey = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_des1 = course_reqs[0][\"course_descr\"]\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1',\n",
    "                          aws_access_key_id=rootkey['AWSAccessKeyId'],\n",
    "                          aws_secret_access_key=rootkey['AWSSecretKey'])\n",
    "\n",
    "print(json.dumps(comprehend.detect_key_phrases(Text=course_des1, LanguageCode='en')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = json.loads(json.dumps(comprehend.detect_key_phrases(Text=course_des1, LanguageCode='en')))\n",
    "for i in range(len(keywords[\"KeyPhrases\"])//2): #only displaying half\n",
    "    keyword = keywords[\"KeyPhrases\"][i][\"Text\"]\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of all Organizations/Clubs at NYU using Engage API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955\n",
      "To help students turn to Christ and live for Christ.180, Fellowship, Christian, Small Groups, Purpose, Christianity\n",
      "180 Fellowship - All-University\n"
     ]
    }
   ],
   "source": [
    "url = 'https://sandbox.api.it.nyu.edu/engage-exp/organizations'\n",
    "headers = {'Authorization': rootkey['NYUAccessKey']}\n",
    "\n",
    "req = requests.get(url, headers=headers)\n",
    "\n",
    "# print(req.status_code)\n",
    "# print(req.headers)\n",
    "# print(req.text)\n",
    "\n",
    "org_reqs = json.loads(req.text)\n",
    "# print(req[0])\n",
    "print(len(req))\n",
    "print(org_reqs[0][\"description\"] + org_reqs[0][\"keywords\"])\n",
    "print(org_reqs[0][\"long_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "\n",
    "def inverseDocumentFrequency(term, allDocuments):\n",
    "    numDocumentsWithThisTerm = 0\n",
    "    for doc in allDocuments:\n",
    "        if term.lower() in allDocuments[doc].lower().split():\n",
    "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n",
    " \n",
    "    if numDocumentsWithThisTerm > 0:\n",
    "        return 1.0 + log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "  '''\n",
    "  Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "  def norm(vector):\n",
    "    return math.sqrt(np.dot(vector, vector))\n",
    "\n",
    "  denominator = (norm(vector1) * norm(vector2))\n",
    "  return np.dot(vector1, vector2) / denominator if denominator != 0 else 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
